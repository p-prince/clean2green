<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction</title>


<meta name="description" content="A diffusion-based pipeline for green-screen correction." />
<meta name="keywords" content="Green-screen, diffusion model, relighting, compositing, Clean2Green" />


<meta property="og:type" content="website" />
<meta property="og:title" content="Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction" />
<meta property="og:description" content="A diffusion-based pipeline for green-screen correction." />
<meta property="og:image" content="./static/images/og-cover.jpg" />
<meta property="og:url" content="https://example.com" />
<meta name="twitter:card" content="summary_large_image" />


<link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,700|Castoro:400,700&display=swap" rel="stylesheet" />


<link rel="stylesheet" href="./static/css/bulma.min.css" />
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
<link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
<link rel="stylesheet" href="./static/css/index.css" />
<link rel="icon" href="./static/images/favicon.svg" />


<script defer src="./static/js/fontawesome.all.min.js"></script>
<script defer src="./static/js/bulma-carousel.min.js"></script>
<script defer src="./static/js/bulma-slider.min.js"></script>


<style>
.publication-title { letter-spacing: .2px; }
.hero video, .section video { width: 100%; height: auto; border-radius: 12px; }
.interpolation-image { border-radius: 10px; }
.icon-link { margin: 0 .5rem; }
footer .content { font-size: .95rem; }
</style>
</head>
<body>


<section class="hero" id="top">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction</h1>


<div class="is-size-6 publication-authors" aria-label="Authors">
<span class="author-block"><a href="#">Prince Odion</a><sup>1</sup>,</span>
<span class="author-block"><a href="#">William A P Smith</a><sup>1</sup>,</span>
<span class="author-block"><a href="#">James Gardner</a><sup>1</sup>,</span>
<span class="author-block"><a href="#">Florian Block</a><sup>1</sup></span>
</div>
<div class="is-size-6 publication-authors">
<span class="author-block"><sup>1</sup>University of York</span>
</div>


<div class="column has-text-centered">
<div class="publication-links" aria-label="Project Links">
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Paper (PDF)">
<span class="icon"><i class="fas fa-file-pdf" aria-hidden="true"></i></span>
<span>Paper</span>
</a>
</span>
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="arXiv">
<span class="icon"><i class="ai ai-arxiv" aria-hidden="true"></i></span>
<span>arXiv</span>
</a>
</span>
<span class="link-block">
<a href="#video" class="external-link button is-normal is-rounded is-dark" aria-label="Demo video link">
<span class="icon"><i class="fab fa-youtube" aria-hidden="true"></i></span>
<span>Video</span>
</a>
</span>
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Source code">
<span class="icon"><i class="fab fa-github" aria-hidden="true"></i></span>
<span>Code</span>
</a>
</span>
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Dataset">
<span class="icon"><i class="far fa-images" aria-hidden="true"></i></span>
<span>Data</span>
</a>
</span>
</div>
</div>


</div>
</div>
</div>
</div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/30rock.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We present a diffusion-based approach that ensures consistent illumination between composited subjects and their target environments, while also producing physically and geometrically coherent shadows.
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <span class="dnerf">Clean2Green</span>, a pipeline for enhancing the realism of green-screen composites. 
            Poor-quality composites often exhibit lighting mismatches, missing shadows, and edge artefacts that break visual coherence. 
            To address this, Clean2Green introduces a dataset generation pipeline that synthesizes paired degraded and clean footage using 
            neural relighting, matting, segmentation, and background reconstruction—enabling supervised training for composite correction.
          </p>
          <p>
            Using this dataset, we fine-tune <span class="dnerf">LTX-Video</span>, a transformer-based latent diffusion model, with 
            Low-Rank Adaptation (LoRA) to restore realistic lighting and shadows. The model produces sharper edges, cleaner details, 
            and perceptually consistent illumination across diverse examples.
          </p>
          <p>
            Clean2Green offers both a scalable framework for data generation and an empirical demonstration that diffusion-based video 
            models can transform flawed composites into visually coherent scenes, advancing automation in professional VFX workflows.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Clean2Green Pipeline</h2>

        <!-- Clean2Green Pipeline Image -->
        <figure class="image">
          <img
            src="./static/images/pipeline.svg"
            alt="Clean2Green pipeline: Input (degraded composite) → Segmentation/Matting → Background reconstruction → Relighting → Output (clean composite)"
            loading="lazy"
            width="1280" height="720"
            style="border-radius:12px;"
          />
          <figcaption class="has-text-centered is-size-6 mt-2">
            Clean2Green Pipeline: synthesizes paired degraded–clean footage for supervised training.
          </figcaption>
        </figure>

        <!-- Pipeline Description -->
        <div class="content has-text-justified mt-4">
          <p>
            Poor-quality green-screen composites and their corresponding clean ground-truth frames
            (a natural-looking video) are not naturally available. To address this gap, we introduce
            the <span class="dnerf">Clean2Green Pipeline</span>, a data generation framework that
            synthesizes paired examples of degraded and realistic footage suitable for supervised
            training of diffusion-based video models.
          </p>
          <p>
            The pipeline combines neural relighting, segmentation and matting, background reconstruction,
            and deliberate degradation steps to simulate common compositing errors such as lighting
            mismatches and missing shadows.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light" id="comparison">
  <div class="hero-body">
    <div class="container is-fluid">
      <h2 class="title is-3 has-text-centered mb-5">Before vs After Results</h2>
      <div id="comparison-carousel" class="carousel results-carousel">

        <!-- Slide 1 -->
        <div class="item">
          <div class="columns is-vcentered">
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/predator_bad.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">Before</p>
            </div>
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/predator_good.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">After</p>
            </div>
          </div>
        </div>

        <!-- Slide 2 -->
        <div class="item">
          <div class="columns is-vcentered">
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/boyz_bad.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">Before</p>
            </div>
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/boyz_good.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">After</p>
            </div>
          </div>
        </div>

        <!-- Slide 3 -->
        <div class="item">
          <div class="columns is-vcentered">
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/joker_bad.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">Before</p>
            </div>
            <div class="column is-half">
              <video autoplay muted loop playsinline controls preload="metadata">
                <source src="./static/videos/pipeline_results/joker_good.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered is-size-6 mt-2">After</p>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Carousel Activation -->
<script defer>
document.addEventListener('DOMContentLoaded', () => {
  bulmaCarousel.attach('#results-carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,
    loop: false,
    pauseOnHover: true
    });
});
</script>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
