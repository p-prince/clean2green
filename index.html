<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction</title>


<meta name="description" content="A diffusion-based pipeline for green-screen correction." />
<meta name="keywords" content="Green-screen, diffusion model, relighting, compositing, Clean2Green" />


<meta property="og:type" content="website" />
<meta property="og:title" content="Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction" />
<meta property="og:description" content="A diffusion-based pipeline for green-screen correction." />
<meta property="og:image" content="./static/images/og-cover.jpg" />
<meta property="og:url" content="https://example.com" />
<meta name="twitter:card" content="summary_large_image" />


<link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,700|Castoro:400,700&display=swap" rel="stylesheet" />


<link rel="stylesheet" href="./static/css/bulma.min.css" />
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
<link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
<link rel="stylesheet" href="./static/css/index.css" />
<link rel="icon" href="./static/images/favicon.svg" />


<script defer src="./static/js/fontawesome.all.min.js"></script>
<script defer src="./static/js/bulma-carousel.min.js"></script>
<script defer src="./static/js/bulma-slider.min.js"></script>


<style>
.publication-title { letter-spacing: .2px; }
.hero video, .section video { width: 100%; height: auto; border-radius: 12px; }
.interpolation-image { border-radius: 10px; }
.icon-link { margin: 0 .5rem; }
footer .content { font-size: .95rem; }
</style>
</head>
<body>


<section class="hero" id="top">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-2 publication-title">Clean2Green: A Diffusion-Based Pipeline for Green-Screen Correction</h1>


<div class="is-size-6 publication-authors" aria-label="Authors">
<span class="author-block"><a href="#">Prince Odion</a>,</span>
<span class="author-block"><a href="https://www-users.york.ac.uk/~waps101/">William A. P. Smith</a>,</span>
<span class="author-block"><a href="https://jadgardner.github.io/">James Gardner</a>,</span>
<span class="author-block"><a href="https://www.florianblock.net/">Florian Block</a></span>
</div>
<div class="is-size-6 publication-authors">
<span class="author-block">University of York</span>
</div>


<div class="column has-text-centered">
<div class="publication-links" aria-label="Project Links">
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Paper (PDF)">
<span class="icon"><i class="fas fa-file-pdf" aria-hidden="true"></i></span>
<span>Paper (Coming Soon)</span>
</a>
</span>
<!-- <span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="arXiv">
<span class="icon"><i class="ai ai-arxiv" aria-hidden="true"></i></span>
<span>arXiv</span>
</a>
</span> -->
<!-- <span class="link-block">
<a href="#video" class="external-link button is-normal is-rounded is-dark" aria-label="Demo video link">
<span class="icon"><i class="fab fa-youtube" aria-hidden="true"></i></span>
<span>Video</span>
</a>
</span> -->
<span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Source code">
<span class="icon"><i class="fab fa-github" aria-hidden="true"></i></span>
<span>Code (Coming Soon)</span>
</a>
</span>
<!-- <span class="link-block">
<a href="#" class="external-link button is-normal is-rounded is-dark" aria-label="Dataset">
<span class="icon"><i class="far fa-images" aria-hidden="true"></i></span>
<span>Data</span>
</a>
</span> -->
</div>
</div>


</div>
</div>
</div>
</div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered" style="position: relative;">
      <div style="position: relative; display: inline-block;">
        <video id="teaser" autoplay muted loop playsinline style="border-radius:12px; width:100%;">
          <source src="./static/videos/30rock.mp4" type="video/mp4">
        </video>

        <!-- Overlay labels -->
        <span class="video-label" style="position:absolute; top:10px; left:10px; background:rgba(0,0,0,0.6); color:white; padding:4px 8px; border-radius:6px; font-size:0.9rem;">
          Original Video
        </span>
        <span class="video-label" style="position:absolute; top:10px; right:10px; background:rgba(0,0,0,0.6); color:white; padding:4px 8px; border-radius:6px; font-size:0.9rem;">
          Ours
        </span>
      </div>

      <h2 class="subtitle mt-4">
        Our diffusion-based model corrects flawed green-screen composites by ensuring consistent illumination between composited subjects and their target environments, while cleaning edge artifacts and producing physically and geometrically coherent shadows.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Greenscreen compositing remains widely used due to its accessibility and cost-effectiveness. However, it suffers from inconsistent lighting between actors and virtual backgrounds. We propose an end-to-end approach that directly transforms badly composited videos into realistic results. Our method consists of two key components: (1) <span class="dnerf">clean2green</span>, a pipeline for generating paired training data by removing actors and their effects from real footage and re-compositing them with inconsistent lighting, and (2) <span class="dnerf">green2clean</span>, a conditional video diffusion model that learns the inverse mapping from bad composites to realistic video.
          </p>

          <p>
            By fine-tuning existing generative models on our synthetic paired data, we achieve high-quality results while requiring only modest training data. Our approach works directly on composited video, enabling <em>remastering</em> of archive footage where original layers are unavailable. As well as realistically relighting actors (with plausible specular reflections, saturations and self-shadowing) our model also correctly predicts cast shadows of the actor onto the scene. We validate our method on <span class="dnerf">green2clean-500</span>, a new benchmark dataset with ground truth, and demonstrate significant improvements on a benchmark of real-world examples of badly composited video.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Approach -->
<section id="method" class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Our Approach</h2>

        <!-- Clean2Green Pipeline Image -->
        <figure class="image">
          <img
            src="./static/images/teaser.png"
            alt="Clean2Green pipeline: Input (degraded composite) → Segmentation/Matting → Background reconstruction → Relighting → Output (clean composite)"
            loading="lazy"
            width="2200" height="1000"
            style="border-radius:12px;"
          />
        </figure>

        
        <!-- Description -->
          <p class="content has-text-justified mt-5">
            We propose a degradation pipeline (clean2green) to transform real videos into versions that appear as if an actor's green-screen performance has been badly composited into the scene. This provides paired training data to fine-tune a generative video model to convert bad composites into realistic video (green2clean). When the trained model is applied to real bad composites, it realistically harmonises the foreground to the background without needing foreground masks, explicit lighting estimation or re-rendering.
          </p>
      </div>
    </div>
  </div>
</section>


<!-- Clean2Green Pipeline -->
<section id="pipeline" class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Clean2Green Pipeline</h2>

        <!-- Clean2Green Pipeline Image -->
        <figure class="image">
          <img
            src="./static/images/pipeline.svg"
            alt="Clean2Green pipeline: Input (degraded composite) → Segmentation/Matting → Background reconstruction → Relighting → Output (clean composite)"
            loading="lazy"
            width="1600" height="900"
            style="border-radius:12px;"
          />
        </figure>

        <!-- Description -->
        <p class="content has-text-justified mt-5">
          The Clean2Green pipeline generates paired clean–degraded video data for supervised relighting model training. 
          It begins with an input video processed through three main modules. In the 
          Relighting Module, the footage is decomposed into geometry buffers (G-Buffers) and 
          relit under diverse random environment maps using the DiffusionRenderer, which performs 
          both inverse and forward rendering to ensure physically consistent illumination. The 
          Segmentation Module employs SAM2 to obtain actor masks and MatAnyone to refine 
          foreground mattes, isolating the subject. Meanwhile, the 
          Background Extraction Module uses Omnimatte to generate an empty background plate. Finally, the relit actor is composited back onto the extracted background 
          to produce realistic paired clean and degraded frames for training.
        </p>
      </div>
    </div>
  </div>
</section>



<!-- Before/After Results -->
<section id="results" class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Clean2Green Samples</h2>

        <div id="results-carousel" class="carousel results-carousel mt-4">

          <!-- Slide 4 -->
          <div class="item">
            <div class="columns is-vcentered">
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/good_1.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Original Video</p>
              </div>
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/bad_1.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Synthetic Degraded Sample</p>
              </div>
            </div>
          </div>

          <!-- Slide 1 -->
          <div class="item">
            <div class="columns is-vcentered">
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/blade_good.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Original Video</p>
              </div>
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/blade_bad.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Synthetic Degraded Sample</p>
              </div>
            </div>
          </div>

          <!-- Slide 2 -->
          <div class="item">
            <div class="columns is-vcentered">
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/boyz_good.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Original Video</p>
              </div>
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/boyz_bad.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Synthetic Degraded Sample</p>
              </div>
            </div>
          </div>

          <!-- Slide 3 -->
          <div class="item">
            <div class="columns is-vcentered">
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/joker_good.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Original Video</p>
              </div>
              <div class="column">
                <video autoplay muted loop playsinline controls preload="metadata">
                  <source src="./static/videos/pipeline_results/joker_bad.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered is-size-6 mt-2">Synthetic Degraded Sample</p>
              </div>
            </div>
          </div>

        </div> <!-- /carousel -->

        <!-- Caption under carousel -->
        <p class="has-text-centered has-text-grey mt-4 is-size-6">
          <span class="dnerf">Clean2Green</span> removes actor-dependent lighting effects such as shadows and reflections, 
          then relights the actor under inconsistent illumination to simulate poor compositing conditions. 
          This process introduces edge artefacts and lighting mismatches that make the actor appear 
          detached from the environment, creating realistic <em>bad composites</em> for training.
        </p>

      </div>
    </div>
  </div>
</section>



<!-- Carousel Activation -->
<script defer>
document.addEventListener('DOMContentLoaded', () => {
  bulmaCarousel.attach('#results-carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,
    loop: false,
    pauseOnHover: true
    });
});
</script>

<!-- Green2Clean Intro Section -->
<section id="green2clean-intro" class="section">
  <div class="container">

    <!-- Title -->
    <h2 class="title is-3 has-text-centered">
      <span class="dnerf">Green2Clean</span>
    </h2>

    <!-- Intro paragraph (left-aligned normal text) -->
    <div class="content subtitle mt-5">
      <p>
        <span class="dnerf">Green2Clean</span> is a <strong>conditional video diffusion model</strong> that learns the inverse
        mapping from <em>bad composites</em> to realistic video. Built on top of the
        <a href="https://github.com/Lightricks/LTX-Video-Trainer/tree/main?tab=readme-ov-file" target="_blank"><strong>LTX</strong></a>
        architecture, it is fine-tuned using <strong>Low-Rank Adaptation (LoRA)</strong> on paired data produced by
        <span class="dnerf">Clean2Green</span>. The model restores consistent illumination, reconstructs shadows, and enhances
        visual coherence across frames to produce temporally stable, photorealistic results.
      </p>
    </div>

  </div>
</section>


<!-- Green2Clean Reconstructions Section -->
<section id="green2clean-recon" class="section">
  <div class="container">

    <!-- Subheading -->
    <h3 class="title is-4 has-text-centered">Green2Clean Reconstructions</h3>
    <p class="has-text-centered is-size-6 has-text-grey mb-5" style="max-width: 850px; margin: 0 auto;">
      Evaluation on synthetically constructed <em>bad composites</em> formed by pairing random foregrounds and backgrounds
      from <strong>VideoMatte240K</strong> (Washington University). The model harmonises foreground–background illumination and colour,
      restoring spatial coherence and generating plausible self- and cast-shadows.
    </p>

    <!-- Carousel -->
    <div id="g2c-carousel" class="carousel results-carousel mt-4" style="max-width: 900px; margin: 0 auto;">

      <!-- Slide A -->
      <div class="item">
        <figure class="image" style="position:relative;border-radius:12px;overflow:hidden;">
          <video autoplay muted loop playsinline controls preload="metadata"
            style="display:block;width:100%;height:auto;max-height:420px;object-fit:cover;">
            <source src="./static/videos/green2clean-washington/step_010000_15.mp4" type="video/mp4">
          </video>
          <span class="tag is-dark" style="position:absolute;top:10px;left:10px;">Random Foreground + Background</span>
          <span class="tag is-dark" style="position:absolute;top:10px;right:10px;">Green2Clean Reconstruction</span>
        </figure>
      </div>

      <!-- Slide B -->
      <div class="item">
        <figure class="image" style="position:relative;border-radius:12px;overflow:hidden;">
          <video autoplay muted loop playsinline controls preload="metadata"
            style="display:block;width:100%;height:auto;max-height:420px;object-fit:cover;">
            <source src="./static/videos/green2clean-washington/5_large_dataset.mp4" type="video/mp4">
          </video>
          <span class="tag is-dark" style="position:absolute;top:10px;left:10px;">Random Foreground + Background</span>
          <span class="tag is-dark" style="position:absolute;top:10px;right:10px;">Green2Clean Reconstruction</span>
        </figure>
      </div>

      <!-- Slide C -->
      <div class="item">
        <figure class="image" style="position:relative;border-radius:12px;overflow:hidden;">
          <video autoplay muted loop playsinline controls preload="metadata"
            style="display:block;width:100%;height:auto;max-height:420px;object-fit:cover;">
            <source src="./static/videos/green2clean-washington/step_010000_10.mp4" type="video/mp4">
          </video>
          <span class="tag is-dark" style="position:absolute;top:10px;left:10px;">Random Foreground + Background</span>
          <span class="tag is-dark" style="position:absolute;top:10px;right:10px;">Green2Clean Reconstruction</span>
        </figure>
      </div>

      <!-- Slide D -->
      <div class="item">
        <figure class="image" style="position:relative;border-radius:12px;overflow:hidden;">
          <video autoplay muted loop playsinline controls preload="metadata"
            style="display:block;width:100%;height:auto;max-height:420px;object-fit:cover;">
            <source src="./static/videos/green2clean-washington/step_010000_9.mp4" type="video/mp4">
          </video>
          <span class="tag is-dark" style="position:absolute;top:10px;left:10px;">Random Foreground + Background</span>
          <span class="tag is-dark" style="position:absolute;top:10px;right:10px;">Green2Clean Reconstruction</span>
        </figure>
      </div>

      <!-- Slide E -->
      <div class="item">
        <figure class="image" style="position:relative;border-radius:12px;overflow:hidden;">
          <video autoplay muted loop playsinline controls preload="metadata"
            style="display:block;width:100%;height:auto;max-height:420px;object-fit:cover;">
            <source src="./static/videos/green2clean-washington/step_010000_7.mp4" type="video/mp4">
          </video>
          <span class="tag is-dark" style="position:absolute;top:10px;left:10px;">Random Foreground + Background</span>
          <span class="tag is-dark" style="position:absolute;top:10px;right:10px;">Green2Clean Reconstruction</span>
        </figure>
      </div>

    </div><!-- /carousel -->

  </div>
</section>


<!-- Optional carousel init (if not already present globally) -->
<script>
  document.addEventListener('DOMContentLoaded', () => {
    if (window.bulmaCarousel && bulmaCarousel.attach) {
      bulmaCarousel.attach('#g2c-carousel', {
        slidesToScroll: 1,
        slidesToShow: 1,
        autoplay: false,
        pauseOnHover: true,
        infinite: true
      });
    }
  });
</script>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Related Work -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Our work builds upon several recent advances in diffusion-based rendering, segmentation, and background reconstruction.
          </p>

          <p>
            For scene relighting, we use <a href="#">Diffusion Renderer</a>.
          </p>

          <p>
            For segmentation and matting, we use <a href="#">SAM&nbsp;2</a> and <a href="#">Mat&nbsp;Anyone</a>.
          </p>

          <p>
            For background reconstruction, we use <a href="#">Gen-Omnimatte</a>.
          </p>

          <p>
            These components together enable the Clean2Green pipeline.
          </p>
        </div>
      </div>
    </div>
    <!-- /Related Work -->

  </div>
</section>

<!-- BibTex 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
!-->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
